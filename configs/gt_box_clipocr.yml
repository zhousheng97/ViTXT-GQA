includes:
- common/defaults/configs/datasets/videoqa/T2S_human.yml
# Use soft copy
dataset_attributes:
  gt_box:
    image_features:
      train:
      - fps10_video_vit_feat 
      val:
      - fps10_video_vit_feat 
      test:
      - fps10_video_vit_feat 
    imdb_files:
      train:
      - vtextgqa/qa_annotation/ViteVQA_0.0.2_t1s2train.npy 
      val:
      - vtextgqa/ground_annotation/qa_sub_val.npy 
      test:
      - vtextgqa/ground_annotation/qa_sub_test.npy 
    ocr_infos:
      train:
      - fps10_ocr_detection_ClipOCR/train 
      val:
      - fps10_ocr_detection_ClipOCR/val
      test:
      - fps10_ocr_detection_ClipOCR/test
    ground_infos:
      val:
      - ground_annotation/annotation_bbox_ClipOCR_recog_t1s2val.npy
      test:
      - ground_annotation/annotation_bbox_ClipOCR_recog_t1s2test.npy
    processors:
      text_processor:
        type: bert_tokenizer
        params:
          max_length: 20
      answer_processor:
        type: m4c_answer
        params:
          vocab_file: m4vitevqa/vocabulary/fixed_vocab_top5k_t1s2.txt
          preprocessor:
            type: simple_word
            params: {}
          context_preprocessor:
            type: simple_word
            params: {}
          max_length: 960 # 240 960   # truncate the total number of [T,N] OCRs
          max_copy_steps: 12
          num_answers: 10
      copy_processor:   
        type: copy
        params:
          max_length: 960  # truncate the total number of OCRs 
      phoc_processor: 
        type: phoc
        params:
          max_length: 960   # truncate the total number of OCRs
    frames: 64 
    ocr_frame_num: 15
    ocr_max_num: 960
model_attributes:
  T2S_human:
    lr_scale_frcn: 0.1
    lr_scale_text_bert: 0.1
    lr_scale_mmt: 1.0  # no scaling
    text_bert_init_from_bert_base: true
    text_bert:
      num_hidden_layers: 3
    obj:
      mmt_in_dim: 1074 # 1024 (ViT) + 50 (Temporal ID)
      dropout_prob: 0.1
    ocr:
      mmt_in_dim: 1004    # 300 (FastText) + 604 (PHOC)  + 50 (Temporal ID) + 50 (Track ID)
      dropout_prob: 0.1
    translayers:
      hidden_size: 768
      num_hidden_layers: 2
    grounding:
      frame_topk: 4 
      ocr_topk: 4 
      max_ocr_num: 960 
      frame_num: 64 
      ocr_frame_num: 15
      hidden_size: 768
    encoder:
      hidden_size: 768
      num_hidden_layers: 2
    mmt:
      hidden_size: 768
      num_hidden_layers: 3
    classifier:
      type: linear
      ocr_max_num: 960 
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params: {}
    model_data_dir: /data/zsheng/Data_T5_ViteVQA/data
    metrics:
    - type: textvqa_accuracy
    - type: stvqa_anls
    - type: IOU@0.3
    - type: IOU@0.5
    - type: GQA@0.3
    - type: GQA@0.5
    losses:
    - type: pos_bce_loss
      weight: 1.0 
      params: {}
optimizer_attributes:
  params:
    eps: 1.0e-08
    lr: 1e-4
    weight_decay: 0
  type: Adam
training_parameters:
    clip_norm_mode: all
    clip_gradients: true
    max_grad_l2_norm: 0.25
    lr_scheduler: true
    lr_steps:
    - 10000
    - 20000
    lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.2
    warmup_iterations: 1000
    max_iterations: 24000
    batch_size: 48
    num_workers: 8
    task_size_proportional_sampling: true
    monitored_metric: gt_box/textvqa_accuracy
    metric_minimize: false
